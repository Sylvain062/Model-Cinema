{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Démarrage du chronomètre pour calculer la durée d'exécution du script\n",
    "start_time = time.time()\n",
    "\n",
    "# Importation du fichier CSV contenant les id\n",
    "df_id = pd.read_csv('Data/df_for_scrap_imdb_filtre4.csv')\n",
    "\n",
    "# Définition de la plage d'id à scraper\n",
    "id_in = 0\n",
    "id_out = len(df_id)\n",
    "id = id_in\n",
    "\n",
    "# Définition du nombre de requêtes effectuées en parallèle (Dépend du CPU)\n",
    "nombre_req = 12\n",
    "\n",
    "# Définition du chemin du fichier csv contenant les id\n",
    "path_out = f'Data/imdb_titres_originaux_{id_in}_{id_out}.csv'\n",
    "\n",
    "# Définition de la variable initial pour pour suivre la progression\n",
    "pourcent = 0\n",
    "\n",
    "# Définition du HEADERS pour que les requetes ne soit pas bloquées\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "\n",
    "# Définition de la fonction pour scraper les données de IMDB\n",
    "def imdb_titre_scrap(i):\n",
    "    global pourcent, id#, time_limit\n",
    "    id += 1\n",
    "    titre = budget = titre_original = None\n",
    "    \n",
    "    try:\n",
    "        # Construction de l'URL pour chaque film basé sur son ID\n",
    "        url = f'https://www.imdb.com/title/{i}'\n",
    "        res = session.get(url, allow_redirects=True, headers=HEADERS)\n",
    "        # Print du % avec une intervalle de 72 tests\n",
    "        pourcent += 1\n",
    "        if pourcent == 10:\n",
    "            print('Execution :', round(((id-id_in)/(id_out-id_in))*100, 2), '%')\n",
    "            pourcent = 0\n",
    "        # Vérification du statut de la réponse, retourne des \"0\" si le code HTTP n'est pas 200 ou si l'URL a été redirigée\n",
    "        if res is None or res.status_code != 200:\n",
    "            return i, \"0\", f\"Code HTTP = {res.status_code}\"\n",
    "        \n",
    "        # Utilisation de BeautifulSoup pour parser le contenu HTML\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        #print(soup)\n",
    "    \n",
    "        titre_original = soup.find('div', class_=lambda x: x and 'sc-d8941411-1' in x)\n",
    "        if titre_original:\n",
    "            titre = titre_original.get_text()\n",
    "            titre = titre.replace('Original title: ', '')\n",
    "        else:\n",
    "            titre_temp = soup.find('title')\n",
    "            titre = titre_temp.get_text() if titre_temp else ''\n",
    "            titre = titre.split(' (')[0].strip()\n",
    "        \n",
    "        budget_filtre1 = soup.find('div', {'data-testid': 'title-boxoffice-section'})\n",
    "        if budget_filtre1 is not None:\n",
    "            budget_balise = budget_filtre1.find('span', class_='ipc-metadata-list-item__list-content-item')\n",
    "            if budget_balise is not None:\n",
    "                budget = budget_balise.text.strip()\n",
    "\n",
    "\n",
    "    # Gestion des exceptions liées aux requêtes HTTP\n",
    "    except requests.exceptions.RequestException:\n",
    "        return i, \"0\", \"Exeption de la requête\"\n",
    "    print (i,' ',titre,' ',budget)\n",
    "    # Retourne les données extraites ou des \"0\" si les données ne sont pas disponibles\n",
    "    resultat = (\n",
    "        i,\n",
    "        titre if titre else \"0\",\n",
    "        budget if budget else \"0\",\n",
    "    )\n",
    "    return resultat\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les données extraites   \n",
    "df = pd.DataFrame(columns=['id', 'titre', 'budget']) \n",
    "session = requests.Session()\n",
    "\n",
    "# Utilisation d'un ThreadPoolExecutor pour exécuter les scrapings en parallèle et utiliser les differents coeurs logique du PC.\n",
    "with ThreadPoolExecutor(max_workers=nombre_req) as executor:\n",
    "\n",
    "    # Soumission des tâches de scraping pour chaque ID de film entre les valeurs id_in et id_out\n",
    "    tests = [executor.submit(imdb_titre_scrap, i) for i in df_id['movie_id'][id_in:id_out]]\n",
    "    for test in tests:\n",
    "        i, titre, budget = test.result()\n",
    "        if titre != \"0\":\n",
    "            df.loc[i] = [i, titre, budget]\n",
    "        else:\n",
    "            print(i,' ', titre, ' ', budget, 'Impossible à enregistrer')\n",
    "\n",
    "# Arrêt du chronomètre et affichage de la durée d'exécution\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Le script a pris {execution_time} secondes pour s'exécuter.\")\n",
    "\n",
    "# Sauvegarde des résultats dans un fichier CSV\n",
    "df.to_csv(path_out, index=False)\n",
    "\n",
    "# Affichage du DataFrame final\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
